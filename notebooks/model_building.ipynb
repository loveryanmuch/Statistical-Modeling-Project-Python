{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        Number_of_Bikes   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                 9.435e-08\n",
      "Date:                Mon, 23 Oct 2023   Prob (F-statistic):               1.00\n",
      "Time:                        07:19:19   Log-Likelihood:            -1.3151e+07\n",
      "No. Observations:             3637500   AIC:                         2.630e+07\n",
      "Df Residuals:                 3637498   BIC:                         2.630e+07\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.2914      0.071    131.473      0.000       9.153       9.430\n",
      "Rating      3.002e-12      0.016   1.84e-10      1.000      -0.032       0.032\n",
      "==============================================================================\n",
      "Omnibus:                  1395074.762   Durbin-Watson:                   0.001\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          5435608.006\n",
      "Skew:                           1.920   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.595   Cond. No.                         68.4\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "data_paris = pd.read_csv(\"C:/Users/17789/Python-Project/data_paris.csv\")\n",
    "data_yelp = pd.read_csv(\"C:/Users/17789/Python-Project/yelp_results.csv\", encoding='latin-1')\n",
    "\n",
    "# Merging the datasets on a common attribute, e.g., 'location_id'\n",
    "merged_data = pd.merge(data_paris, data_yelp, how=\"cross\")\n",
    "\n",
    "# Assuming 'rating' is the column name for store ratings in data3 and 'num_bikes' for number of bicycles in data1\n",
    "X = merged_data['Rating']\n",
    "y = merged_data['Number_of_Bikes']\n",
    "\n",
    "# Adding a constant to the model (it's a best practice!)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Building the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Getting the summary of the regression\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide model output and an interpretation of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Dep. Variable (Number_of_Bikes): This is what we're trying to predict. We want to know if the 'Rating' of a store can help us guess the 'Number_of_Bikes' nearby.\n",
    "\n",
    "# 2) R-squared (0.000): This is a measure of how well our model fits the data. A value of 0.000 means our model doesn't really explain the variation in the number of bikes based on store ratings. \n",
    "\n",
    "# 3) coef (for Rating: 3.002e-12): This tells us about the relationship between store 'Rating' and 'Number_of_Bikes'. The tiny number (almost zero) suggests that a change in store rating doesn't really affect the number of bikes nearby.\n",
    "\n",
    "# 4) P>|t| (for Rating: 1.000): This is a probability value. It helps us see if the relationship (coef) is by chance. A value of 1.000 means there's no significant relationship between store rating and the number of bikes.\n",
    "\n",
    "# 5) const (9.2914): This is the starting point or baseline number of bikes when the rating is zero. But since ratings can't be zero, think of it as a starting reference.\n",
    "\n",
    "# 6) Omnibus and Jaque-Bera: These are statistical tests related to the data's distribution. In simple terms, they indicate that there might be some issues with the data being non-normal.\n",
    "\n",
    "# 7) Durbin-Watson: This checks for patterns in the residuals (the differences between the observed and predicted values). A value close to 0.001 suggests there might be some patterns we're missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you turn the regression model into a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "PerfectSeparationError",
     "evalue": "Perfect separation detected, results not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPerfectSeparationError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\17789\\Python-Project\\notebooks\\model_building.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/Python-Project/notebooks/model_building.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m X \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39madd_constant(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/Python-Project/notebooks/model_building.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Building the logistic regression model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/17789/Python-Project/notebooks/model_building.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39mLogit(y, X)\u001b[39m.\u001b[39mfit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/Python-Project/notebooks/model_building.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Getting the summary of the regression\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/17789/Python-Project/notebooks/model_building.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1983\u001b[0m, in \u001b[0;36mLogit.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m   1980\u001b[0m \u001b[39m@Appender\u001b[39m(DiscreteModel\u001b[39m.\u001b[39mfit\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m)\n\u001b[0;32m   1981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, start_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton\u001b[39m\u001b[39m'\u001b[39m, maxiter\u001b[39m=\u001b[39m\u001b[39m35\u001b[39m,\n\u001b[0;32m   1982\u001b[0m         full_output\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, disp\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1983\u001b[0m     bnryfit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(start_params\u001b[39m=\u001b[39mstart_params,\n\u001b[0;32m   1984\u001b[0m                           method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m   1985\u001b[0m                           maxiter\u001b[39m=\u001b[39mmaxiter,\n\u001b[0;32m   1986\u001b[0m                           full_output\u001b[39m=\u001b[39mfull_output,\n\u001b[0;32m   1987\u001b[0m                           disp\u001b[39m=\u001b[39mdisp,\n\u001b[0;32m   1988\u001b[0m                           callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m   1989\u001b[0m                           \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1991\u001b[0m     discretefit \u001b[39m=\u001b[39m LogitResults(\u001b[39mself\u001b[39m, bnryfit)\n\u001b[0;32m   1992\u001b[0m     \u001b[39mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:230\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m mlefit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(start_params\u001b[39m=\u001b[39mstart_params,\n\u001b[0;32m    231\u001b[0m                      method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    232\u001b[0m                      maxiter\u001b[39m=\u001b[39mmaxiter,\n\u001b[0;32m    233\u001b[0m                      full_output\u001b[39m=\u001b[39mfull_output,\n\u001b[0;32m    234\u001b[0m                      disp\u001b[39m=\u001b[39mdisp,\n\u001b[0;32m    235\u001b[0m                      callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    236\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m mlefit\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:563\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_t\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    562\u001b[0m optimizer \u001b[39m=\u001b[39m Optimizer()\n\u001b[1;32m--> 563\u001b[0m xopt, retvals, optim_settings \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39m_fit(f, score, start_params,\n\u001b[0;32m    564\u001b[0m                                                fargs, kwargs,\n\u001b[0;32m    565\u001b[0m                                                hessian\u001b[39m=\u001b[39mhess,\n\u001b[0;32m    566\u001b[0m                                                method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    567\u001b[0m                                                disp\u001b[39m=\u001b[39mdisp,\n\u001b[0;32m    568\u001b[0m                                                maxiter\u001b[39m=\u001b[39mmaxiter,\n\u001b[0;32m    569\u001b[0m                                                callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    570\u001b[0m                                                retall\u001b[39m=\u001b[39mretall,\n\u001b[0;32m    571\u001b[0m                                                full_output\u001b[39m=\u001b[39mfull_output)\n\u001b[0;32m    572\u001b[0m \u001b[39m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    573\u001b[0m optim_settings\u001b[39m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:241\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    238\u001b[0m     fit_funcs\u001b[39m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    240\u001b[0m func \u001b[39m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 241\u001b[0m xopt, retvals \u001b[39m=\u001b[39m func(objective, gradient, start_params, fargs, kwargs,\n\u001b[0;32m    242\u001b[0m                      disp\u001b[39m=\u001b[39mdisp, maxiter\u001b[39m=\u001b[39mmaxiter, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    243\u001b[0m                      retall\u001b[39m=\u001b[39mretall, full_output\u001b[39m=\u001b[39mfull_output,\n\u001b[0;32m    244\u001b[0m                      hess\u001b[39m=\u001b[39mhessian)\n\u001b[0;32m    246\u001b[0m optim_settings \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m: method, \u001b[39m'\u001b[39m\u001b[39mstart_params\u001b[39m\u001b[39m'\u001b[39m: start_params,\n\u001b[0;32m    247\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: maxiter, \u001b[39m'\u001b[39m\u001b[39mfull_output\u001b[39m\u001b[39m'\u001b[39m: full_output,\n\u001b[0;32m    248\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp, \u001b[39m'\u001b[39m\u001b[39mfargs\u001b[39m\u001b[39m'\u001b[39m: fargs, \u001b[39m'\u001b[39m\u001b[39mcallback\u001b[39m\u001b[39m'\u001b[39m: callback,\n\u001b[0;32m    249\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mretall\u001b[39m\u001b[39m'\u001b[39m: retall, \u001b[39m\"\u001b[39m\u001b[39mextra_fit_funcs\u001b[39m\u001b[39m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    250\u001b[0m optim_settings\u001b[39m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:443\u001b[0m, in \u001b[0;36m_fit_newton\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess, ridge_factor)\u001b[0m\n\u001b[0;32m    441\u001b[0m         history\u001b[39m.\u001b[39mappend(newparams)\n\u001b[0;32m    442\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m         callback(newparams)\n\u001b[0;32m    444\u001b[0m     iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    445\u001b[0m fval \u001b[39m=\u001b[39m f(newparams, \u001b[39m*\u001b[39mfargs)  \u001b[39m# this is the negative likelihood\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\17789\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:214\u001b[0m, in \u001b[0;36mDiscreteModel._check_perfect_pred\u001b[1;34m(self, params, *args)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraise_on_perfect_prediction \u001b[39mand\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         np\u001b[39m.\u001b[39mallclose(fittedvalues \u001b[39m-\u001b[39m endog, \u001b[39m0\u001b[39m)):\n\u001b[0;32m    213\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPerfect separation detected, results not available\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m PerfectSeparationError(msg)\n",
      "\u001b[1;31mPerfectSeparationError\u001b[0m: Perfect separation detected, results not available"
     ]
    }
   ],
   "source": [
    "# To convert the regression model into a classification model, \n",
    "# need to change the target variable (y) from a continuous variable to a categorical one. \n",
    "# One common approach is to bin the continuous target variable into categories.\n",
    "\n",
    "# Therefore, the 'number of bicycles' was classified into two categories, 'high' and 'low', as shown below.\n",
    "# TFor example, you can determine the median and then classify values above this specific value as 'high' and values below this specific value as 'low'.\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Data Loading\n",
    "data_paris = pd.read_csv(\"C:/Users/17789/Python-Project/data_paris.csv\")\n",
    "data_yelp = pd.read_csv(\"C:/Users/17789/Python-Project/yelp_results.csv\", encoding='latin-1')\n",
    "\n",
    "# Merging the datasets on a common attribute, e.g., 'location_id'\n",
    "merged_data = pd.merge(data_paris, data_yelp, on=['Latitude', 'Longitude'], how='right')\n",
    "\n",
    "# Convert 'Number_of_Bikes' to a binary categorical variable\n",
    "threshold = merged_data['Number_of_Bikes'].median()\n",
    "merged_data['Bike_Category'] = [1 if i > threshold else 0 for i in merged_data['Number_of_Bikes']]\n",
    "\n",
    "# Assuming 'rating' is the column name for store ratings in data_yelp\n",
    "y = merged_data['Bike_Category']\n",
    "X = merged_data[['Review Count', 'Rating']]\n",
    "\n",
    "# Adding a constant to the model (it's a best practice!)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Building the logistic regression model\n",
    "model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Getting the summary of the regression\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
